# app/services/file_rag_store.py

# íƒ€ì… íŒíŠ¸ìš©
from typing import List, Dict, Any, Optional

# íŒŒì¼ ë° ê²½ë¡œ ì‘ì—…ìš©
import os

# JSON ì €ì¥/ë¡œë“œìš©
import json

# ìˆ«ì ë²¡í„° ì—°ì‚°ìš©
import numpy as np

# PDF ì½ê¸°ìš©
from pypdf import PdfReader

# FAISS ë²¡í„° ì¸ë±ìŠ¤
import faiss

# ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from app.core.config import get_settings

# Ollama ì„ë² ë”© ì„œë¹„ìŠ¤
from app.services.embeddings import embedding_service

# ë¡œê¹…
from app.core.logging import get_logger

logger = get_logger(__name__)


class FileRagVectorStore:
    """uploads í´ë” ë¬¸ì„œë¥¼ ì„ë² ë”©í•´ì„œ
    FAISS ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬í•˜ê³  ê²€ìƒ‰ê¹Œì§€ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    # ì—…ë¡œë“œ ë¬¸ì„œìš© FAISS ì¸ë±ìŠ¤ ê¸°ë³¸ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•œë‹¤.
    def __init__(self):
        """ì„¤ì •ê³¼ FAISS ì¸ë±ìŠ¤ ê¸°ë³¸ ìƒíƒœë¥¼ ì´ˆê¸°í™”"""
        # ì„¤ì • ë¡œë“œ
        self.settings = get_settings()

        # ì„ë² ë”© ì°¨ì› ìˆ˜ (ì˜ˆ: 1024)
        self.dimension: int = embedding_service.dimension

        # FAISS ì¸ë±ìŠ¤ (ì²˜ìŒì—” None)
        self.index: Optional[faiss.IndexFlatIP] = None

        # ì¸ë±ìŠ¤ì— ë“¤ì–´ê°„ ê° ë²¡í„°ì™€ ë§¤í•‘ë˜ëŠ” í…ìŠ¤íŠ¸ / ë©”íƒ€ë°ì´í„°
        self.texts: List[str] = []
        self.metadatas: List[Dict[str, Any]] = []

        logger.info(
            f"RagVectorStore ì´ˆê¸°í™”: store_path={self.settings.UPLOADS_VECTOR_INDEX_PATH}"
        )

    # íŠ¹ì • íŒŒì¼ì˜ ë¬¸ì„œë¥¼ ë¹„ë™ê¸°ë¡œ ë¡œë“œí•˜ëŠ” í›…(í˜„ì¬ëŠ” í…œí”Œë¦¿)ì´ë‹¤.
    async def load_documents(self, file_path: str):
        logger.info(f"ë¬¸ì„œ ë¡œë“œ ì‹œì‘: {file_path}")
        try:
            documents = []
            # ë¡œë“œ ë¡œì§...
            logger.info(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ì²­í¬")
            return documents
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    # ì—…ë¡œë“œ/ì¸ë±ìŠ¤ìš© ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤.
    def _ensure_dirs(self) -> None:
        """uploads, data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±"""
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.settings.UPLOADS_DIR, exist_ok=True)

        # ì¸ë±ìŠ¤ / ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ë“¤ì–´ê°ˆ ìƒìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
        index_dir = os.path.dirname(self.settings.UPLOADS_VECTOR_INDEX_PATH)
        if index_dir:
            os.makedirs(index_dir, exist_ok=True)

    # ë‹¨ì¼ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.
    def _load_text_from_file(self, path: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ íŒŒì¼ì„ ì½ì–´ í…ìŠ¤íŠ¸ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

        ë°˜í™˜ í˜•íƒœ ì˜ˆ:
        [
                {"text": "...", "meta": {"source": "a.pdf", "page": 0, "type": "pdf"}},
                ...
        ]
        """
        results: List[Dict[str, Any]] = []

        # í™•ì¥ì ì†Œë¬¸ìë¡œ
        ext = os.path.splitext(path)[1].lower()

        # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš°(.txt, .md)
        if ext in [".txt", ".md"]:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()

            # ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
            if content.strip():
                results.append(
                    {
                        "text": content,
                        "meta": {
                            "source": os.path.basename(path),
                            "type": "text",
                        },
                    }
                )
            return results

        # PDF íŒŒì¼ì¸ ê²½ìš°
        if ext == ".pdf":
            reader = PdfReader(path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text() or ""
                if not text.strip():
                    continue
                results.append(
                    {
                        "text": text,
                        "meta": {
                            "source": os.path.basename(path),
                            "type": "pdf",
                            "page": page_num,
                        },
                    }
                )
            return results

        # ê·¸ ì™¸ í¬ë§·ì€ ì•„ì§ ì§€ì› X
        return []

    # uploads í´ë”ì˜ íŒŒì¼ë“¤ë¡œë¶€í„° ìƒˆ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•œë‹¤.
    def _build_from_uploads(self) -> None:
        """uploads ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ë¡œë¶€í„° ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±"""
        # ë””ë ‰í† ë¦¬ ì¤€ë¹„
        self._ensure_dirs()

        all_texts: List[str] = []
        all_metas: List[Dict[str, Any]] = []

        # uploads í´ë” ì•ˆì˜ íŒŒì¼ë“¤ì„ ìˆœíšŒ
        for filename in os.listdir(self.settings.UPLOADS_DIR):
            path = os.path.join(self.settings.UPLOADS_DIR, filename)

            # ë””ë ‰í† ë¦¬ëŠ” ìŠ¤í‚µ
            if not os.path.isfile(path):
                continue

            # íŒŒì¼ì—ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            entries = self._load_text_from_file(path)
            for entry in entries:
                all_texts.append(entry["text"])
                all_metas.append(entry["meta"])

        # ì²­í¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í‚µ
        if not all_texts:
            logger.info("âš ï¸ uploads ë””ë ‰í† ë¦¬ì— ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"ğŸ“„ ì´ {len(all_texts)} ê°œì˜ ì²­í¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤...")

        # Ollama ì„ë² ë”©ìœ¼ë¡œ ë²¡í„° ìƒì„±
        embeddings = np.ascontiguousarray(
            embedding_service.embed_texts(all_texts), dtype="float32"
        )

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì“°ê¸° ìœ„í•´ L2 ì •ê·œí™”
        faiss.normalize_L2(embeddings)

        # FAISS ë‚´ì  ê¸°ë°˜ ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatIP(self.dimension)

        # ë²¡í„° ì¶”ê°€
        index.add(embeddings)

        # ë©”ëª¨ë¦¬ì— ë³´ê´€
        self.index = index
        self.texts = all_texts
        self.metadatas = all_metas

        # ë””ìŠ¤í¬ì— ì €ì¥
        self._save_index()

    # í˜„ì¬ ë©”ëª¨ë¦¬ì˜ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•œë‹¤.
    def _save_index(self) -> None:
        """í˜„ì¬ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        # ì¸ë±ìŠ¤ê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
        if self.index is None:
            return

        # ë””ë ‰í† ë¦¬ ì¤€ë¹„
        self._ensure_dirs()

        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, self.settings.UPLOADS_VECTOR_INDEX_PATH)

        # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        meta_data = {
            "texts": self.texts,
            "metadatas": self.metadatas,
        }
        with open(self.settings.UPLOADS_VECTOR_META_PATH, "w", encoding="utf-8") as f:
            json.dump(meta_data, f, ensure_ascii=False)

        logger.info(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.settings.UPLOADS_VECTOR_INDEX_PATH}")

    # ë””ìŠ¤í¬ì— ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•œë‹¤.
    def _load_index(self) -> None:
        """ë””ìŠ¤í¬ì—ì„œ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œ"""
        # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ ë¡œë“œ ë¶ˆê°€
        if not (
            os.path.exists(self.settings.UPLOADS_VECTOR_INDEX_PATH)
            and os.path.exists(self.settings.UPLOADS_VECTOR_META_PATH)
        ):
            logger.info(
                "âš ï¸ ì¸ë±ìŠ¤ ë˜ëŠ” ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ë¹Œë“œí•´ì•¼ í•©ë‹ˆë‹¤."
            )
            return

        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        index = faiss.read_index(self.settings.UPLOADS_VECTOR_INDEX_PATH)

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(self.settings.UPLOADS_VECTOR_META_PATH, "r", encoding="utf-8") as f:
            meta_data = json.load(f)

        self.index = index
        self.texts = meta_data.get("texts", [])
        self.metadatas = meta_data.get("metadatas", [])

        logger.info(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.texts)} ê°œ ì²­í¬")

    # í•„ìš” ì‹œ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ë¹Œë“œí•´ì„œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì¤€ë¹„í•œë‹¤.
    async def ensure_vector_store(self):
        logger.info("ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸...")
        if self.index is None:
            logger.info(
                f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘: {self.settings.UPLOADS_VECTOR_INDEX_PATH}"
            )
            # ì¸ë±ìŠ¤/ë©”íƒ€ íŒŒì¼ì´ ë‘˜ ë‹¤ ìˆìœ¼ë©´ ë¡œë“œ
            if os.path.exists(
                self.settings.UPLOADS_VECTOR_INDEX_PATH
            ) and os.path.exists(self.settings.UPLOADS_VECTOR_META_PATH):
                logger.info("ğŸ“¦ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
                self._load_index()
            else:
                logger.info("ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë¯€ë¡œ uploads ë””ë ‰í† ë¦¬ì—ì„œ ìƒˆë¡œ ë¹Œë“œí•©ë‹ˆë‹¤...")
                self._build_from_uploads()
            logger.info("ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        else:
            logger.debug("âœ“ ë²¡í„° ìŠ¤í† ì–´ ì´ë¯¸ ì´ˆê¸°í™”ë¨")

    # ì¿¼ë¦¬ë¥¼ ì„ë² ë”© í›„ FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ìƒìœ„ ì²­í¬ë“¤ì„ ì¡°íšŒí•œë‹¤.
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ ë¬¸ìì—´ì„ ì„ë² ë”©í•˜ì—¬ FAISSì—ì„œ top_k ìœ ì‚¬í•œ ì²­í¬ë¥¼ ë°˜í™˜"""
        # ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì˜ˆì™¸
        if self.index is None:
            raise RuntimeError("ë²¡í„° ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vec = embedding_service.embed_query(query)

        # ì •ê·œí™”
        faiss.normalize_L2(query_vec)

        # FAISS ê²€ìƒ‰
        scores, indices = self.index.search(query_vec, top_k)
        scores = scores[0]
        indices = indices[0]

        results: List[Dict[str, Any]] = []

        for score, idx in zip(scores, indices):
            if idx < 0 or idx >= len(self.texts):
                continue
            results.append(
                {
                    "score": float(score),
                    "text": self.texts[idx],
                    "meta": self.metadatas[idx],
                }
            )

        return results


# ì•± ì „ì²´ì—ì„œ ê³µìœ í•´ì„œ ì“¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
file_rag_vector_store = FileRagVectorStore()
